import os
import requests
import random
import time
import re
import urllib.parse
import xml.etree.ElementTree as ET # ğŸ‘ˆ êµ¬ê¸€ ë‰´ìŠ¤ íŒŒì‹±ìš© (ê¸°ë³¸ ë‚´ì¥)
from groq import Groq

# 1. í™˜ê²½ ë³€ìˆ˜
webhook_url = os.environ.get("DISCORD_WEBHOOK_URL")
groq_key = os.environ.get("GROQ_API_KEY")

client_groq = Groq(api_key=groq_key)

SPORTS_CATEGORIES = {
    "âš½ SOCCER": [("soccer/eng.1", "ğŸ‡¬ğŸ‡§ EPL"), ("soccer/uefa.champions", "ğŸ‡ªğŸ‡º UCL")],
    "ğŸ€ BASKETBALL": [("basketball/nba", "ğŸ‡ºğŸ‡¸ NBA")],
    "âš¾ BASEBALL": [("baseball/mlb", "ğŸ‡ºğŸ‡¸ MLB")]
}

# ---------------------------------------------------------
# ğŸ“¡ 1. ê²½ê¸° ì¼ì • ìˆ˜ì§‘ (ESPN)
# ---------------------------------------------------------
def fetch_matches_by_category(endpoints):
    headers = {"User-Agent": "Mozilla/5.0"}
    category_matches = []
    
    for sport_path, icon in endpoints:
        url = f"https://site.api.espn.com/apis/site/v2/sports/{sport_path}/scoreboard"
        try:
            res = requests.get(url, headers=headers, timeout=5)
            data = res.json()
            for event in data.get('events', []):
                state = event.get('status', {}).get('type', {}).get('state', '')
                if state == 'pre': 
                    name = event.get('name', 'Unknown')
                    category_matches.append(f"{icon} {name}")
        except:
            continue
            
    return list(set(category_matches))

# ---------------------------------------------------------
# ğŸ“° 2. êµ¬ê¸€ ë‰´ìŠ¤ RSS í•´í‚¹ (ë§‰í˜ ì—†ëŠ” íŒ©íŠ¸ ìˆ˜ì§‘)
# ---------------------------------------------------------
def fetch_google_news(match_name):
    print(f"ğŸ“° [{match_name}] êµ¬ê¸€ ë‰´ìŠ¤ RSS ìŠ¤ìº” ì¤‘...")
    
    # ê²€ìƒ‰ì–´ ì •ì œ (ì˜ˆ: "ğŸ‡¬ğŸ‡§ EPL Manchester United vs West Ham" -> "Manchester United West Ham")
    clean_name = re.sub(r'[^\w\s]', ' ', match_name).replace('EPL', '').replace('NBA', '').replace('MLB', '').strip()
    
    # ê²€ìƒ‰ ì¿¼ë¦¬: íŒ€ ì´ë¦„ + ë¶€ìƒ(injury) ë˜ëŠ” í”„ë¦¬ë·°(preview)
    query = urllib.parse.quote(f"{clean_name} injury OR preview OR news")
    
    # êµ¬ê¸€ ë‰´ìŠ¤ RSS ì£¼ì†Œ (ì˜ì–´ ê¸°ì‚¬ê°€ íŒ©íŠ¸ê°€ ê°€ì¥ ì •í™•í•¨)
    url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
    
    news_context = ""
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        root = ET.fromstring(res.text)
        
        # ìƒìœ„ 4ê°œ ê¸°ì‚¬ ì œëª©ê³¼ ë°œí–‰ì¼ ê°€ì ¸ì˜¤ê¸°
        items = root.findall('.//item')[:4]
        for idx, item in enumerate(items):
            title = item.find('title').text
            pub_date = item.find('pubDate').text
            news_context += f"- [{pub_date}] {title}\n"
            
    except Exception as e:
        print(f"âš ï¸ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
    if not news_context.strip():
        news_context = "ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ì§€ì‹ê³¼ ì „ë ¥ ìœ„ì£¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
        
    print(f"âœ… ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°:\n{news_context}")
    return news_context

# ---------------------------------------------------------
# ğŸ§  3. AI ë¶„ì„ (ì‹¤ì‹œê°„ íŒ©íŠ¸ ì£¼ì…)
# ---------------------------------------------------------
def get_ai_analysis(target, category_name, news_data):
    print(f"ğŸ§  AI ë¶„ì„ ì‹œì‘...")
    model = "llama-3.3-70b-versatile"
    
    prompt = f"""
    Target Match: {target}
    Category: {category_name}
    Role: Professional Sports Betting Analyst.
    
    ğŸš¨ [LIVE NEWS DATA] ğŸš¨
    Read these latest news headlines regarding the match:
    {news_data}
    
    Task: 
    1. Base your analysis HEAVILY on the news provided above.
    2. Explicitly mention injuries, manager quotes, or team form found in the headlines.
    3. Do NOT invent information not present in the news or your established knowledge base.
    
    Format Structure:
    
    ===TITLE===
    (Match Title)
    
    ===KR===
    1. ğŸ“° ì‹¤ì‹œê°„ íŒ©íŠ¸: (ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ìµœì‹  ì´ìŠˆ ìš”ì•½)
    2. ğŸ“‰ ì–‘ íŒ€ ê¸°ì„¸: (ìƒìŠ¹ì„¸/í•˜ë½ì„¸ ë¶„ì„)
    3. ğŸƒ ìŠ¹ë¶€ì²˜: (ë‰´ìŠ¤ë¥¼ ë°˜ì˜í•œ ì „ìˆ ì  í•µì‹¬)
    4. ğŸ˜ˆ ì•…ë§ˆì˜ ì†ì‚­ì„: (ë°°ë‹¹ í•¨ì •ì´ë‚˜ ìˆ¨ê²¨ì§„ ë¦¬ìŠ¤í¬)
    5. ğŸ’° ìµœì¢… í”½: (ìŠ¹íŒ¨/ì–¸ì˜¤ë²„)
    
    ===EN===
    1. Live Fact Check: ...
    2. Team Momentum: ...
    3. Crucial Point: ...
    4. Devil's Whisper: ...
    5. Final Pick: ...
    
    ===ZH===
    1. å®æ—¶åˆ†æ: ...
    2. çƒé˜Ÿæ°”åŠ¿: ...
    3. å…³é”®ç‚¹: ...
    4. æ¶é­”ä½è¯­: ...
    5. æœ€ç»ˆé¢„æµ‹: ...
    
    ===END===
    """
    
    try:
        response = client_groq.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=model,
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"âŒ AI ì—ëŸ¬: {e}")
        return None

# ---------------------------------------------------------
# âœ‚ï¸ 4. ë°ì´í„° ê°€ê³µ
# ---------------------------------------------------------
def parse_text_to_data(text):
    data = {}
    try:
        if "===TITLE===" in text:
            data['title'] = text.split("===TITLE===")[1].split("===KR===")[0].strip()
        else:
            data['title'] = "Match Analysis"
            
        if "===KR===" in text:
            data['kr'] = text.split("===KR===")[1].split("===EN===")[0].strip()
            
        if "===EN===" in text:
            data['en'] = text.split("===EN===")[1].split("===ZH===")[0].strip()
            
        if "===ZH===" in text:
            data['zh'] = text.split("===ZH===")[1].split("===END===")[0].strip()
            
        return data
    except:
        return {"title": "Error", "kr": text, "en": "-", "zh": "-"}

# ---------------------------------------------------------
# ğŸš€ 5. ë©”ì¸ ë£¨í”„
# ---------------------------------------------------------
def run():
    print("ğŸš€ [System] AI Sports Edge (Google News RAG Edition) Started...")
    
    for category_name, endpoints in SPORTS_CATEGORIES.items():
        print(f"\nğŸ” Searching for {category_name}...")
        
        matches = fetch_matches_by_category(endpoints)
        
        if not matches:
            print(f"   ğŸ’¤ {category_name}: ì˜ˆì •ëœ ê²½ê¸° ì—†ìŒ.")
            continue 
            
        target = random.choice(matches)
        print(f"   âœ… Target Found: {target}")
        
        # ğŸ’¡ êµ¬ê¸€ ë‰´ìŠ¤ RSS ìŠ¤ìº”
        news_data = fetch_google_news(target)
        
        raw_text = get_ai_analysis(target, category_name, news_data)
        if not raw_text: continue
        
        data = parse_text_to_data(raw_text)
        
        embed = {
            "title": f"ğŸ† {category_name} Pick: {data.get('title')}",
            "color": 3447003,
            "fields": [
                {"name": "ğŸ‡°ğŸ‡· í•œêµ­ì–´ (ë‰´ìŠ¤ ê¸°ë°˜ ë¶„ì„)", "value": data.get('kr', '-'), "inline": False},
                {"name": "ğŸ‡ºğŸ‡¸ English Report", "value": data.get('en', '-'), "inline": False},
                {"name": "ğŸ‡¨ğŸ‡³ ä¸­æ–‡æŠ¥å‘Š", "value": data.get('zh', '-'), "inline": False}
            ],
            "footer": {"text": "Powered by ESPN & Google News RSS â€¢ AI Sports Edge"}
        }
        
        payload = {"embeds": [embed]}
        
        if webhook_url:
            try:
                requests.post(webhook_url, json=payload)
                print(f"   ğŸš€ {category_name} ë¦¬í¬íŠ¸ ì „ì†¡ ì™„ë£Œ!")
            except Exception as e:
                print(f"   âŒ ì „ì†¡ ì‹¤íŒ¨: {e}")
        
        time.sleep(5)

    print("\nğŸ [System] All Jobs Finished.")

if __name__ == "__main__":
    run()
